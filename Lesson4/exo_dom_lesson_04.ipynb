{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lxml.html\n",
    "import requests\n",
    "import getpass\n",
    "import math\n",
    "import re\n",
    "\n",
    "def get_model(text):\n",
    "    if 'zen' in text:\n",
    "        return 'zen'\n",
    "    elif 'intens' in text:\n",
    "        return 'intens'\n",
    "    elif 'life' in text:\n",
    "        return 'life'\n",
    "    return None\n",
    "\n",
    "def get_spec_car_data(text):\n",
    "    URL = \"https://www.leboncoin.fr/voitures/\" + text + \".htm?ca=12_s\"\n",
    "    r = requests.get(URL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    data = soup.find_all('h2', class_=re.compile('clearfix'))\n",
    "    data_cleaned = [d.text.replace(\"\\n\", \"\")\n",
    "                    .replace(\"\\xa0€\", \"\")\n",
    "                    .replace(\"Prix\", \"\")\n",
    "                    .replace(\" \", \"\")\n",
    "                    .replace(\"Kilométrage\", \"\")\n",
    "                    .replace(\"KM\", \"\")\n",
    "                    .replace(\"Année-modèle\", \"\") for d in data]\n",
    "    price = data_cleaned[0]\n",
    "    year = data_cleaned[4]\n",
    "    km = data_cleaned[5]\n",
    "    \n",
    "    phone = [\"NoNumber\"]\n",
    "    description = soup.find_all('p', itemprop=re.compile('description'))\n",
    "    description_cleaned = [re.findall(r\"(\\d{2}[\\.\\s]??\\d{2}[\\.\\s]??\\d{2}[\\.\\s]??\\d{2}[\\.\\s]??\\d{2}[\\.\\s]??)\",\n",
    "                                      d.text.strip()) for d in description]\n",
    "    phone.extend(description_cleaned[0])\n",
    "    return price + \"|\" + year + \"|\" + km + \"|\" + phone[-1].replace('.', '').replace(' ', '')\n",
    "\n",
    "def get_cote(df):\n",
    "    year = df.Year\n",
    "    version = df.Version_Cleaned\n",
    "    \n",
    "    if version==None:\n",
    "        return None\n",
    "    \n",
    "    URL = ('https://www.lacentrale.fr/cote-auto-renault-zoe-' + version + '+charge+rapide-' + str(year) + '.html')\n",
    "    r = requests.get(URL)\n",
    "    \n",
    "    '''if r!=200:\n",
    "        print(r)\n",
    "        return None'''\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    data = soup.find_all('span', class_=re.compile('jsRefinedQuot'))\n",
    "    cote_cleaned = int(data[0].text.replace(\"\\n\", \"\").replace(\"\\xa0€\", \"\").replace(\" \", \"\"))\n",
    "    return cote_cleaned\n",
    "    \n",
    "    \n",
    "def get_part_pro_dataframe(region):\n",
    "    p = Pool(3)\n",
    "    frames = []\n",
    "\n",
    "    for part_pro_link in ['c', 'p']:\n",
    "        page = 1\n",
    "        id_all_links = []\n",
    "        all_versions = []\n",
    "        \n",
    "        URL = ('https://www.leboncoin.fr/voitures/offres/' + region + '/?o=' + str(page)\n",
    "           + '&q=renault%20zo%E9&f=' + part_pro_link)\n",
    "        r = requests.get(URL)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        id_links = re.findall('voitures/(\\d+).htm', str(soup))\n",
    "        versions = soup.find_all('h2',attrs={\"class\": u\"item_title\"})\n",
    "        versions_cleaned = [v.text.replace('\\n', '').replace('\\t', '').lower()\n",
    "                            .replace('renault', '').replace('zoe', '').strip()\n",
    "                            for v in versions]\n",
    "\n",
    "        id_all_links = id_links\n",
    "        all_versions = versions_cleaned\n",
    "\n",
    "\n",
    "        while(id_links != []):\n",
    "            page = page + 1\n",
    "            URL = ('https://www.leboncoin.fr/voitures/offres/' + region + '/?o=' + str(page)\n",
    "                   + '&q=renault%20zo%E9&f=' + part_pro_link)\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            id_links = re.findall('voitures/(\\d+).htm', str(soup))\n",
    "            id_all_links.extend(id_links)\n",
    "            \n",
    "            versions = soup.find_all('h2',attrs={\"class\": u\"item_title\"})\n",
    "            versions_cleaned = [v.text.replace('\\n', '').replace('\\t', '').lower()\n",
    "                                .replace('renault', '').replace('zoe', '').strip()\n",
    "                                for v in versions]\n",
    "            all_versions.extend(versions_cleaned)\n",
    "        \n",
    "        if part_pro_link == \"c\":\n",
    "            part_pro = \"pro\"\n",
    "        else:\n",
    "            part_pro = \"part\"\n",
    "            \n",
    "            \n",
    "        data = {\"ID_Link\": id_all_links, \"Version\": all_versions, \"Seller\": part_pro, \"Region\": region}\n",
    "        df = pd.DataFrame(data)\n",
    "        frames.append(df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "    df[\"Version_Cleaned\"] = p.map(get_model, df.Version.tolist())\n",
    "    df[\"Info\"] = p.map(get_spec_car_data, df.ID_Link.tolist())\n",
    "    df[\"Price\"] = df.Info.str.split('|',3, expand=True)[0].tolist()\n",
    "    df[\"Year\"] = df.Info.str.split('|',3, expand=True)[1].tolist()\n",
    "    df[\"Kilometer\"] = df.Info.str.split('|',3, expand=True)[2].tolist()\n",
    "    df[\"Phone\"] = df.Info.str.split('|',3, expand=True)[3].tolist()\n",
    "    df[\"Cote\"] = df.apply(get_cote, axis=1)\n",
    "        \n",
    "    return df\n",
    "## OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 1.49 s, total: 16.2 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_leboncoin_data_frame(region):\n",
    "    dict_df = {}\n",
    "    all_df = []\n",
    "    for r in region:\n",
    "        dict_df[r] = get_part_pro_dataframe(region=r)\n",
    "        all_df.append(dict_df[r])\n",
    "    return pd.concat(all_df)\n",
    "\n",
    "df_leboncoin = get_leboncoin_data_frame([\"ile_de_france\", \"aquitaine\", \"provence_alpes_cote_d_azur\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3_KitDataScience",
   "language": "python",
   "name": "py3_kitdatascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
